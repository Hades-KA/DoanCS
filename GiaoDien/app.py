import streamlit as st
import os
import pdfplumber
import re
import base64
from transformers import pipeline
import pandas as pd
from concurrent.futures import ThreadPoolExecutor
import spacy

# Cáº¥u hÃ¬nh trang Streamlit.
st.set_page_config(page_title="Há»‡ thá»‘ng Há»— trá»£ Tuyá»ƒn dá»¥ng báº±ng AI", layout="wide")

# ThÆ° má»¥c lÆ°u file upload.
UPLOAD_FOLDER = './uploaded_cvs/'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Táº£i mÃ´ hÃ¬nh zero-shot-classification vÃ  cache láº¡i.
@st.cache_resource(show_spinner=True)
def load_classifier():
    try:
        return pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
    except Exception as e:
        st.error(f"Lá»—i khi táº£i mÃ´ hÃ¬nh AI: {str(e)}. Vui lÃ²ng kiá»ƒm tra káº¿t ná»‘i máº¡ng hoáº·c thá»­ láº¡i sau.")
        return None

classifier = load_classifier()
FIELDS = ["Frontend Development", "Backend Development", "Data Science/AI", "DevOps", "Mobile Development"]

# Táº£i mÃ´ hÃ¬nh spaCy â€“ dÃ¹ng Ä‘á»ƒ nháº­n diá»‡n tÃªn á»©ng viÃªn.
@st.cache_resource(show_spinner=True)
def load_spacy_model():
    try:
        nlp = spacy.load("en_core_web_sm")
        return nlp
    except Exception as e:
        st.error("KhÃ´ng táº£i Ä‘Æ°á»£c mÃ´ hÃ¬nh spaCy: " + str(e))
        return None

nlp_spacy = load_spacy_model()

############################################
# CÃ¡c hÃ m xá»­ lÃ½ file vÃ  trÃ­ch xuáº¥t thÃ´ng tin
############################################

def save_uploadedfile(uploadedfile):
    """LÆ°u file upload vÃ o thÆ° má»¥c UPLOAD_FOLDER."""
    path = os.path.join(UPLOAD_FOLDER, uploadedfile.name)
    with open(path, "wb") as f:
        f.write(uploadedfile.getbuffer())
    return path

def extract_text_from_pdf(file_path):
    """TrÃ­ch xuáº¥t text tá»« file PDF (chá»‰ láº¥y tá»‘i Ä‘a 3 trang Ä‘áº§u)."""
    try:
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages[:3]:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text.strip()
    except Exception as e:
        st.error(f"Lá»—i khi Ä‘á»c file PDF: {str(e)}")
        return ""

###############################
# CÃC HÃ€M TRÃCH XUáº¤T TÃŠN á»¨NG VIÃŠN
###############################

def extract_name_first_line(text):
    """
    BÆ°á»›c 0: Náº¿u dÃ²ng Ä‘áº§u tiÃªn cá»§a vÄƒn báº£n trÃ´ng giá»‘ng nhÆ° tÃªn á»©ng viÃªn, 
    tráº£ vá» dÃ²ng Ä‘áº§u tiÃªn Ä‘Ã³.
    TiÃªu chÃ­: khÃ´ng quÃ¡ 50 kÃ½ tá»±, chá»©a Ã­t nháº¥t 2 tá»« vÃ  khÃ´ng cÃ³ email hay sá»‘.
    """
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    if lines:
        first_line = lines[0]
        if len(first_line) <= 50 and len(first_line.split()) >= 2 and not re.search(r'\S+@\S+|\d', first_line):
            return first_line
    return None

def extract_name_spacy_full(text):
    """
    Chia toÃ n bá»™ vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n (chunk) vá»›i kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh vÃ  sá»­ dá»¥ng spaCy
    Ä‘á»ƒ nháº­n diá»‡n táº¥t cáº£ cÃ¡c entity PERSON. Tráº£ vá» candidate xuáº¥t hiá»‡n sá»›m nháº¥t, 
    vá»›i Ä‘iá»u kiá»‡n cÃ³ Ã­t nháº¥t 2 tá»« vÃ  khÃ´ng chá»©a sá»‘.
    """
    chunk_size = 3000
    candidates = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i+chunk_size]
        doc = nlp_spacy(chunk)
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                candidate = ent.text.strip()
                if len(candidate.split()) >= 2 and not re.search(r'\d', candidate):
                    candidates.append((i + ent.start_char, candidate))
    if candidates:
        candidates.sort(key=lambda x: x[0])
        return candidates[0][1]
    return None

def extract_name_heuristic(text):
    """
    QuÃ©t toÃ n bá»™ cÃ¡c dÃ²ng trong vÄƒn báº£n vÃ  chá»n tÃªn á»©ng viÃªn dá»±a trÃªn cÃ¡c tiÃªu chÃ­:
      - Äá»™ dÃ i dÃ²ng dÆ°á»›i 50 kÃ½ tá»±.
      - Ãt nháº¥t 2 tá»«.
      - KhÃ´ng chá»©a email hoáº·c sá»‘ liá»‡u.
      - Tá»· lá»‡ tá»« in hoa >= 50%.
    Tráº£ vá» candidate xuáº¥t hiá»‡n sá»›m nháº¥t.
    """
    lines = text.splitlines()
    candidates = []
    for idx, line in enumerate(lines):
        line = line.strip()
        if len(line) > 50 or re.search(r'\S+@\S+|\d{3,}', line.lower()):
            continue
        words = line.split()
        if len(words) < 2:
            continue
        count_capitalized = sum(1 for w in words if w and w[0].isupper())
        if count_capitalized / len(words) >= 0.5:
            candidates.append((idx, line))
    if candidates:
        candidates.sort(key=lambda x: x[0])
        return candidates[0][1]
    return None

def extract_name_improved(text):
    """
    Cáº£i thiá»‡n nháº­n diá»‡n tÃªn á»©ng viÃªn theo 4 bÆ°á»›c:
      0. Kiá»ƒm tra dÃ²ng Ä‘áº§u tiÃªn cá»§a vÄƒn báº£n.
      1. DÃ¹ng Regex: TÃ¬m dÃ²ng chá»©a "Name:" hoáº·c "TÃªn:" vÃ  láº¥y pháº§n sau dáº¥u phÃ¢n cÃ¡ch.
      2. Sá»­ dá»¥ng spaCy trÃªn toÃ n vÄƒn (chia thÃ nh cÃ¡c chunk) Ä‘á»ƒ nháº­n diá»‡n cÃ¡c entity PERSON.
      3. Náº¿u váº«n khÃ´ng tÃ¬m Ä‘Æ°á»£c, dÃ¹ng fallback heuristic quÃ©t toÃ n bá»™ cÃ¡c dÃ²ng.
    Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c káº¿t quáº£, tráº£ vá» "KhÃ´ng rÃµ".
    """
    # BÆ°á»›c 0: Kiá»ƒm tra dÃ²ng Ä‘áº§u tiÃªn.
    candidate_first = extract_name_first_line(text)
    if candidate_first:
        return candidate_first

    # BÆ°á»›c 1: Sá»­ dá»¥ng Regex.
    regex_match = re.search(r'(Name|TÃªn)\s*[:\-]\s*(.+)', text, re.IGNORECASE)
    if regex_match:
        candidate = regex_match.group(2).split("\n")[0].strip()
        lower_candidate = candidate.lower()
        if candidate and not re.search(r'\S+@\S+|phone|\d', lower_candidate):
            return candidate

    # BÆ°á»›c 2: Sá»­ dá»¥ng spaCy vá»›i chia chunk toÃ n vÄƒn.
    candidate_spacy = extract_name_spacy_full(text)
    if candidate_spacy:
        return candidate_spacy

    # BÆ°á»›c 3: Fallback heuristic quÃ©t toÃ n bá»™ cÃ¡c dÃ²ng.
    candidate_heuristic = extract_name_heuristic(text)
    if candidate_heuristic:
        return candidate_heuristic

    return "KhÃ´ng rÃµ"

############################################
# CÃC HÃ€M KHÃC (SKILL, PDF,...) 
############################################

def normalize_skill(skill):
    """Chuáº©n hÃ³a tÃªn ká»¹ nÄƒng vá» dáº¡ng lowercase vÃ  loáº¡i bá» kÃ½ tá»± thá»«a."""
    return skill.lower().replace("asp.net core", "asp.net").replace(".net core", ".net").replace("(", "").replace(")", "").strip()

def match_skills_accurately(candidate_skills, expected_skills):
    """
    So sÃ¡nh danh sÃ¡ch ká»¹ nÄƒng cá»§a á»©ng viÃªn vá»›i danh sÃ¡ch ká»¹ nÄƒng mong Ä‘á»£i.
    Tráº£ vá»: danh sÃ¡ch ká»¹ nÄƒng trÃ¹ng khá»›p, ká»¹ nÄƒng cÃ²n thiáº¿u vÃ  % bao phá»§.
    """
    norm_candidate = [normalize_skill(s) for s in candidate_skills]
    norm_expected = [normalize_skill(s) for s in expected_skills]
    matched = [s for s, norm_s in zip(expected_skills, norm_expected)
               if any(norm_s in c for c in norm_candidate)]
    missing = [s for s in expected_skills if s not in matched]
    coverage = round(len(matched) / len(expected_skills) * 100, 2) if expected_skills else 0
    return matched, missing, coverage

def extract_skills_list(text):
    """
    TrÃ­ch xuáº¥t cÃ¡c ká»¹ nÄƒng tá»« cÃ¡c dÃ²ng chá»©a tá»« khÃ³a nhÆ° "skill", "tools", "tech", "technology", hoáº·c "framework".
    Sá»­ dá»¥ng regex Ä‘á»ƒ tÃ¡ch cÃ¡c ká»¹ nÄƒng Ä‘Æ°á»£c liá»‡t kÃª sau dáº¥u phÃ¢n cÃ¡ch.
    """
    skills = []
    for line in text.splitlines():
        if re.search(r'(skill|tools|tech|technology|framework)', line, re.IGNORECASE):
            parts = re.split(r'[:,]', line, maxsplit=1)
            if len(parts) > 1:
                items = re.split(r'[,/]', parts[1])
                items = [item.strip(" ()").strip() for item in items if item.strip()]
                skills.extend(items)
    return list(set(skills))

def extract_skills_list_improved(text):
    """
    TrÃ­ch xuáº¥t cÃ¡c ká»¹ nÄƒng tá»« vÄƒn báº£n CV chá»‰ dá»±a trÃªn ná»™i dung cá»§a file,
    khÃ´ng bá»• sung thÃªm tá»« danh sÃ¡ch ká»¹ nÄƒng cá»©ng cÃ³ sáºµn.
    """
    return extract_skills_list(text)

def extract_skills_from_projects(text):
    """
    TrÃ­ch xuáº¥t ká»¹ nÄƒng hoáº·c cÃ´ng nghá»‡ tá»« cÃ¡c Ä‘oáº¡n mÃ´ táº£ dá»± Ã¡n cÃ³ chá»©a cÃ¡c tá»« khÃ³a nhÆ° "project" hoáº·c "dá»± Ã¡n".
    """
    sections = re.findall(r"(?i)(project|dá»± Ã¡n)[^\n]*\n+(.*?)(?=\n{2,}|\Z)", text, re.DOTALL)
    all_skills = set()
    for _, section in sections:
        lines = section.split('\n')
        for line in lines:
            if any(kw in line.lower() for kw in ['stack', 'tech', 'technology', 'tools', 'framework', 'sá»­ dá»¥ng']):
                items = re.split(r'[:,]', line, maxsplit=1)
                if len(items) > 1:
                    for part in re.split(r'[,/â€¢]', items[1]):
                        skill = part.strip(" -â€¢()")
                        if 1 < len(skill) <= 30:
                            all_skills.add(skill)
    return sorted(all_skills)

def predict_field(text_cv):
    """
    Sá»­ dá»¥ng mÃ´ hÃ¬nh zero-shot Ä‘á»ƒ dá»± Ä‘oÃ¡n máº£ng IT dá»±a trÃªn ná»™i dung cá»§a CV tiÃªu chÃ­.
    """
    if classifier is None:
        return "KhÃ´ng xÃ¡c Ä‘á»‹nh"
    short_text = text_cv[:1000]
    result = classifier(short_text, candidate_labels=FIELDS)
    return result['labels'][0]

def display_pdf(file_path):
    """Hiá»ƒn thá»‹ file PDF trong Streamlit thÃ´ng qua iframe."""
    try:
        with open(file_path, "rb") as f:
            base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="700" height="1000" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)
    except Exception as e:
        st.error(f"Lá»—i hiá»ƒn thá»‹ PDF: {e}")

def process_cv(file_path, expected_skills, target_field):
    """
    Xá»­ lÃ½ má»™t file CV:
      - TrÃ­ch xuáº¥t ná»™i dung PDF.
      - XÃ¡c Ä‘á»‹nh tÃªn á»©ng viÃªn báº±ng extract_name_improved.
      - TrÃ­ch xuáº¥t cÃ¡c ká»¹ nÄƒng tá»« toÃ n bá»™ CV vÃ  tá»« pháº§n mÃ´ táº£ dá»± Ã¡n.
      - So khá»›p danh sÃ¡ch ká»¹ nÄƒng vá»›i expected_skills vÃ  tÃ­nh pháº§n trÄƒm bao phá»§.
    """
    text = extract_text_from_pdf(file_path)
    if text:
        name = extract_name_improved(text)
        candidate_skills = extract_skills_list_improved(text)
        project_skills = extract_skills_from_projects(text)
        total_skills = list(set(candidate_skills + project_skills))
        matched, missing, skill_coverage = match_skills_accurately(total_skills, expected_skills)
        if skill_coverage == 0:
            return None
        result_status = "PhÃ¹ há»£p" if skill_coverage >= 50 else "KhÃ´ng phÃ¹ há»£p"
        return {
            'TÃªn file': os.path.basename(file_path),
            'TÃªn á»©ng viÃªn': name,
            'Máº£ng IT': target_field,
            'Pháº§n trÄƒm phÃ¹ há»£p': skill_coverage,
            'Káº¿t quáº£': result_status,
            'Ká»¹ nÄƒng phÃ¹ há»£p': ', '.join(matched),
            'Ká»¹ nÄƒng cÃ²n thiáº¿u': ', '.join(missing),
            'Ká»¹ nÄƒng trong project': ', '.join(project_skills)
        }
    return None

@st.cache_data(show_spinner=True)
def analyze_cvs(uploaded_paths, expected_skills, target_field):
    """
    PhÃ¢n tÃ­ch cÃ¡c file CV sá»­ dá»¥ng ThreadPoolExecutor Ä‘á»ƒ xá»­ lÃ½ song song.
    Cáº­p nháº­t progress bar vÃ  tráº£ vá» káº¿t quáº£ dÆ°á»›i dáº¡ng DataFrame.
    """
    results = []
    warnings = []
    progress_bar = st.progress(0)
    total_files = len(uploaded_paths)
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_cv, path, expected_skills, target_field) for path in uploaded_paths]
        for i, future in enumerate(futures):
            result = future.result()
            if result:
                results.append(result)
            else:
                warnings.append(f"âš ï¸ CV táº¡i {uploaded_paths[i]} khÃ´ng Ä‘áº¡t tiÃªu chÃ­ vÃ  Ä‘Ã£ bá»‹ loáº¡i bá».")
            progress_bar.progress((i + 1) / total_files)
    
    if warnings:
        st.warning(f"âš ï¸ CÃ³ {len(warnings)} CV Ä‘Ã£ bá»‹ loáº¡i bá» do khÃ´ng Ä‘áº¡t tiÃªu chÃ­.")
        with st.expander("Xem chi tiáº¿t cÃ¡c cáº£nh bÃ¡o"):
            st.write("\n".join(warnings))
    return pd.DataFrame(results)

############################################
# GIAO DIá»†N CHÃNH cÃ¹ng Streamlit
############################################

def main():
    st.title("ğŸ“„ Há»‡ thá»‘ng Há»— trá»£ Quáº£n lÃ½ Tuyá»ƒn dá»¥ng báº±ng AI")
    st.sidebar.header("ğŸ“„ Upload CV")
    
    sample_cv_file = st.sidebar.file_uploader("ğŸ“Œ Táº£i lÃªn CV tiÃªu chÃ­ (PDF)", type="pdf")
    uploaded_files = st.sidebar.file_uploader("ğŸ“… Táº£i lÃªn cÃ¡c CV á»©ng viÃªn (PDF)", type=["pdf"], accept_multiple_files=True)
    
    if sample_cv_file and uploaded_files:
        sample_cv_path = save_uploadedfile(sample_cv_file)
        sample_cv_text = extract_text_from_pdf(sample_cv_path)
        # TrÃ­ch xuáº¥t ká»¹ nÄƒng tá»« CV tiÃªu chÃ­ dá»±a trÃªn ná»™i dung file.
        expected_skills = extract_skills_list_improved(sample_cv_text)
        target_field = predict_field(sample_cv_text)
    
        uploaded_paths = [save_uploadedfile(uploaded_file) for uploaded_file in uploaded_files]
        st.sidebar.success(f"âœ… ÄÃ£ upload {len(uploaded_files)} CV á»©ng viÃªn.")
    
        st.success("âœ… Äang tiáº¿n hÃ nh phÃ¢n tÃ­ch CV...")
        df = analyze_cvs(uploaded_paths, expected_skills, target_field)
        st.subheader("ğŸ“Š TÃ³m táº¯t káº¿t quáº£")
        st.success(f"âœ… ÄÃ£ phÃ¢n tÃ­ch {len(df)} CV há»£p lá»‡ trÃªn tá»•ng sá»‘ {len(uploaded_files)} CV.")
    
        if df.empty:
            st.warning("âš ï¸ KhÃ´ng cÃ³ CV nÃ o phÃ¹ há»£p vá»›i tiÃªu chÃ­.")
        else:
            st.subheader("ğŸ“‹ Danh sÃ¡ch á»©ng viÃªn phÃ¹ há»£p")
            df.index = df.index + 1
            st.dataframe(df)
    
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                label="ğŸ“… Táº£i danh sÃ¡ch á»©ng viÃªn Ä‘Ã£ Ä‘Ã¡nh giÃ¡",
                data=csv,
                file_name='cv_filtered_results.csv',
                mime='text/csv',
            )
    
            st.subheader("ğŸ” Xem chi tiáº¿t tá»«ng CV")
            selected_file = st.selectbox("Chá»n má»™t file CV Ä‘á»ƒ xem chi tiáº¿t:", df['TÃªn file'].tolist())
            if selected_file:
                selected_path = next((path for path in uploaded_paths if os.path.basename(path) == selected_file), None)
                if selected_path:
                    text = extract_text_from_pdf(selected_path)
                    if text:
                        st.write(f"### PhÃ¢n tÃ­ch chi tiáº¿t CV: {selected_file}")
                        st.write(f"- **TÃªn á»©ng viÃªn**: {extract_name_improved(text)}")
                        st.write("### Ná»™i dung CV")
                        display_pdf(selected_path)
                        candidate_skills = extract_skills_list_improved(text)
                        project_skills = extract_skills_from_projects(text)
                        total_skills = list(set(candidate_skills + project_skills))
                        matched, missing, skill_coverage = match_skills_accurately(total_skills, expected_skills)
                        st.write("### Tá»‰ lá»‡ phÃ¹ há»£p")
                        st.write(f"- **Tá»•ng**: {skill_coverage}%")
                        st.write("### Ká»¹ nÄƒng phÃ¹ há»£p")
                        st.write(", ".join(matched) if matched else "KhÃ´ng rÃµ")
                        st.write("### Ká»¹ nÄƒng cÃ²n thiáº¿u")
                        st.write(", ".join(missing) if missing else "KhÃ´ng rÃµ")
                        st.write("### Ká»¹ nÄƒng trong project")
                        st.write(", ".join(project_skills) if project_skills else "KhÃ´ng rÃµ")
    
if __name__ == "__main__":
    main()